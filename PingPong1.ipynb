{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled44.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPi/m3w6OTP6LjcDGOiC16h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS767/blob/main/PingPong1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg3pfTqXoUxx"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWpEfAcko4dG",
        "outputId": "86c89789-093f-4e1f-9185-fa57fa9e6452"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Roms.rar', <http.client.HTTPMessage at 0x7f3e3072ff50>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sql33Dw-o-rv",
        "outputId": "3adf6579-70c0-42dc-ca24-5b541dcf2b2c"
      },
      "source": [
        "!pip install unrar\n",
        "!unrar x Roms.rar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unrar in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "Extracting  HC ROMS.zip                                                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  ROMS.zip                                                     \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7BH8Q9WpUOu",
        "outputId": "8aaa8664-1525-49e5-c018-38bbae44e180"
      },
      "source": [
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘rars’: File exists\n",
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN-jErg3qemj"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkRg5YMxqvW8",
        "outputId": "0bf5aed2-1190-4f17-dfcf-14af1d2040b7"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f3e2e0a8810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpHs-F1Brha4"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def resize_frame(frame):\n",
        "    frame = frame[30:-12,5:-4]\n",
        "    frame = np.average(frame,axis = 2)\n",
        "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
        "    frame = np.array(frame,dtype = np.uint8)\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ejw_XRQr5Gi"
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "class Memory():\n",
        "    def __init__(self,max_len):\n",
        "        self.max_len = max_len\n",
        "        self.frames = deque(maxlen = max_len)\n",
        "        self.actions = deque(maxlen = max_len)\n",
        "        self.rewards = deque(maxlen = max_len)\n",
        "        self.done_flags = deque(maxlen = max_len)\n",
        "\n",
        "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal):\n",
        "        self.frames.append(next_frame)\n",
        "        self.actions.append(next_action)\n",
        "        self.rewards.append(next_frames_reward)\n",
        "        self.done_flags.append(next_frame_terminal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tki1w391sNmp"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def initialize_new_game(name, env, agent):\n",
        "    \"\"\"We don't want an agents past game influencing its new game, so we add in some dummy data to initialize\"\"\"\n",
        "    \n",
        "    env.reset()\n",
        "    starting_frame = resize_frame(env.step(0)[0])\n",
        "\n",
        "    dummy_action = 0\n",
        "    dummy_reward = 0\n",
        "    dummy_done = False\n",
        "    for i in range(3):\n",
        "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done)\n",
        "\n",
        "def make_env(name, agent):\n",
        "    env = gym.make(name)\n",
        "    return env\n",
        "\n",
        "def take_step(name, env, agent, score, debug):\n",
        "    \n",
        "    #1 and 2: Update timesteps and save weights\n",
        "    agent.total_timesteps += 1\n",
        "    if agent.total_timesteps % 50000 == 0:\n",
        "      agent.model.save_weights('recent_weights.hdf5')\n",
        "      print('\\nWeights saved!')\n",
        "\n",
        "    #3: Take action\n",
        "    next_frame, next_frames_reward, next_frame_terminal, info = env.step(agent.memory.actions[-1])\n",
        "    \n",
        "    #4: Get next state\n",
        "    next_frame = resize_frame(next_frame)\n",
        "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]\n",
        "    new_state = np.moveaxis(new_state,0,2)/255 #We have to do this to get it into keras's goofy format of [batch_size,rows,columns,channels]\n",
        "    new_state = np.expand_dims(new_state,0) #^^^\n",
        "    \n",
        "    #5: Get next action, using next state\n",
        "    next_action = agent.get_action(new_state)\n",
        "\n",
        "    #6: If game is over, return the score\n",
        "    if next_frame_terminal:\n",
        "        agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
        "        return (score + next_frames_reward),True\n",
        "\n",
        "    #7: Now we add the next experience to memory\n",
        "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
        "\n",
        "    #8: If we are trying to debug this then render\n",
        "    if debug:\n",
        "        env.render()\n",
        "\n",
        "    #9: If the threshold memory is satisfied, make the agent learn from memory\n",
        "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
        "        agent.learn(debug)\n",
        "\n",
        "    return (score + next_frames_reward),False\n",
        "\n",
        "def play_episode(name, env, agent, debug = False):\n",
        "    initialize_new_game(name, env, agent)\n",
        "    done = False\n",
        "    score = 0\n",
        "    while True:\n",
        "        score,done = take_step(name,env,agent,score, debug)\n",
        "        if done:\n",
        "            break\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JteeSBftLcI"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate, starting_lives = 5, debug = False):\n",
        "        self.memory = Memory(max_mem_len)\n",
        "        self.possible_actions = possible_actions\n",
        "        self.epsilon = starting_epsilon\n",
        "        self.epsilon_decay = .9/100000\n",
        "        self.epsilon_min = .05\n",
        "        self.gamma = .95\n",
        "        self.learn_rate = learn_rate\n",
        "        self.model = self._build_model()\n",
        "        self.model_target = clone_model(self.model)\n",
        "        self.total_timesteps = 0\n",
        "        self.lives = starting_lives #this parameter does not apply to pong\n",
        "        self.starting_mem_len = starting_mem_len\n",
        "        self.learns = 0\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Input((84,84,4)))\n",
        "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
        "        optimizer = Adam(self.learn_rate)\n",
        "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
        "        model.summary()\n",
        "        print('\\nAgent Initialized\\n')\n",
        "        return model\n",
        "\n",
        "    def get_action(self,state):\n",
        "        \"\"\"Explore\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.sample(self.possible_actions,1)[0]\n",
        "\n",
        "        \"\"\"Do Best Acton\"\"\"\n",
        "        a_index = np.argmax(self.model.predict(state))\n",
        "        return self.possible_actions[a_index]\n",
        "\n",
        "    def _index_valid(self,index):\n",
        "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def learn(self,debug = False):\n",
        "        \"\"\"we want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
        "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
        "\n",
        "        \"\"\"First we need 32 random valid indicies\"\"\"\n",
        "        states = []\n",
        "        next_states = []\n",
        "        actions_taken = []\n",
        "        next_rewards = []\n",
        "        next_done_flags = []\n",
        "\n",
        "        while len(states) < 32:\n",
        "            index = np.random.randint(4,len(self.memory.frames) - 1)\n",
        "            if self._index_valid(index):\n",
        "                state = [self.memory.frames[index-3], self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index]]\n",
        "                state = np.moveaxis(state,0,2)/255\n",
        "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index], self.memory.frames[index+1]]\n",
        "                next_state = np.moveaxis(next_state,0,2)/255\n",
        "\n",
        "                states.append(state)\n",
        "                next_states.append(next_state)\n",
        "                actions_taken.append(self.memory.actions[index])\n",
        "                next_rewards.append(self.memory.rewards[index+1])\n",
        "                next_done_flags.append(self.memory.done_flags[index+1])\n",
        "\n",
        "        \"\"\"Now we get the ouputs from our model, and the target model. We need this for our target in the error function\"\"\"\n",
        "        labels = self.model.predict(np.array(states))\n",
        "        next_state_values = self.model_target.predict(np.array(next_states))\n",
        "        \n",
        "        \"\"\"Now we define our labels, or what the output should have been\n",
        "           We want the output[action_taken] to be R_(t+1) + Qmax_(t+1) \"\"\"\n",
        "        for i in range(32):\n",
        "            action = self.possible_actions.index(actions_taken[i])\n",
        "            labels[i][action] = next_rewards[i] + (not next_done_flags[i]) * self.gamma * max(next_state_values[i])\n",
        "\n",
        "        \"\"\"Train our model using the states and outputs generated\"\"\"\n",
        "        self.model.fit(np.array(states),labels,batch_size = 32, epochs = 1, verbose = 0)\n",
        "\n",
        "        \"\"\"Decrease epsilon and update how many times our agent has learned\"\"\"\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "        self.learns += 1\n",
        "        \n",
        "        \"\"\"Every 10000 learned, copy our model weights to our target model\"\"\"\n",
        "        if self.learns % 10000 == 0:\n",
        "            self.model_target.set_weights(self.model.get_weights())\n",
        "            print('\\nTarget model updated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2wd27jWHtea_",
        "outputId": "ea2050fb-2184-4100-f0fa-0083b8c36420"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "name = 'PongDeterministic-v4'\n",
        "\n",
        "agent = Agent(possible_actions=[0,2,3],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .00025)\n",
        "env = make_env(name,agent)\n",
        "\n",
        "last_100_avg = [-21]\n",
        "scores = deque(maxlen = 100)\n",
        "max_score = -21\n",
        "\n",
        "\"\"\" If testing:\n",
        "agent.model.load_weights('recent_weights.hdf5')\n",
        "agent.model_target.load_weights('recent_weights.hdf5')\n",
        "agent.epsilon = 0.0\n",
        "\"\"\"\n",
        "\n",
        "env.reset()\n",
        "\n",
        "for i in range(1000000):\n",
        "    timesteps = agent.total_timesteps\n",
        "    timee = time.time()\n",
        "    score = play_episode(name, env, agent, debug = False) #set debug to true for rendering\n",
        "    scores.append(score)\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "\n",
        "    print('\\nEpisode: ' + str(i))\n",
        "    print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
        "    print('Duration: ' + str(time.time() - timee))\n",
        "    print('Score: ' + str(score))\n",
        "    print('Max Score: ' + str(max_score))\n",
        "    print('Epsilon: ' + str(agent.epsilon))\n",
        "\n",
        "    if i%100==0 and i!=0:\n",
        "        last_100_avg.append(sum(scores)/len(scores))\n",
        "        plt.plot(np.arange(0,i+1,100),last_100_avg)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 1,685,667\n",
            "Trainable params: 1,685,667\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Agent Initialized\n",
            "\n",
            "\n",
            "Episode: 0\n",
            "Steps: 823\n",
            "Duration: 1.3435111045837402\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 1\n",
            "Steps: 823\n",
            "Duration: 1.3666894435882568\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 2\n",
            "Steps: 978\n",
            "Duration: 1.604710578918457\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 3\n",
            "Steps: 918\n",
            "Duration: 1.4789021015167236\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 4\n",
            "Steps: 1125\n",
            "Duration: 1.8736979961395264\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 5\n",
            "Steps: 978\n",
            "Duration: 1.5733559131622314\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 6\n",
            "Steps: 883\n",
            "Duration: 1.4460029602050781\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 7\n",
            "Steps: 885\n",
            "Duration: 1.473097324371338\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 8\n",
            "Steps: 1067\n",
            "Duration: 1.7146265506744385\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 9\n",
            "Steps: 1005\n",
            "Duration: 1.6650991439819336\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 10\n",
            "Steps: 1009\n",
            "Duration: 1.6781129837036133\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 11\n",
            "Steps: 1128\n",
            "Duration: 1.870370626449585\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 12\n",
            "Steps: 945\n",
            "Duration: 1.6304690837860107\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 13\n",
            "Steps: 1073\n",
            "Duration: 1.8137786388397217\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 14\n",
            "Steps: 1003\n",
            "Duration: 1.721203327178955\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 15\n",
            "Steps: 1125\n",
            "Duration: 1.8864977359771729\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 16\n",
            "Steps: 1004\n",
            "Duration: 1.744375467300415\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 17\n",
            "Steps: 945\n",
            "Duration: 1.6115496158599854\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 18\n",
            "Steps: 1129\n",
            "Duration: 1.8868341445922852\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 19\n",
            "Steps: 919\n",
            "Duration: 1.5068130493164062\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 20\n",
            "Steps: 922\n",
            "Duration: 1.5636131763458252\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 21\n",
            "Steps: 885\n",
            "Duration: 1.4785692691802979\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 22\n",
            "Steps: 885\n",
            "Duration: 1.4797914028167725\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 23\n",
            "Steps: 943\n",
            "Duration: 1.6278269290924072\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 24\n",
            "Steps: 1172\n",
            "Duration: 1.9667494297027588\n",
            "Score: -18.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 25\n",
            "Steps: 1245\n",
            "Duration: 1.9957776069641113\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 26\n",
            "Steps: 1014\n",
            "Duration: 1.6272327899932861\n",
            "Score: -19.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 27\n",
            "Steps: 1136\n",
            "Duration: 1.8509817123413086\n",
            "Score: -19.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 28\n",
            "Steps: 1013\n",
            "Duration: 1.6212761402130127\n",
            "Score: -19.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 29\n",
            "Steps: 883\n",
            "Duration: 1.4243760108947754\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 30\n",
            "Steps: 883\n",
            "Duration: 1.4383232593536377\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 31\n",
            "Steps: 945\n",
            "Duration: 1.510019302368164\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 32\n",
            "Steps: 883\n",
            "Duration: 1.4221036434173584\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 33\n",
            "Steps: 1064\n",
            "Duration: 1.744215726852417\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 34\n",
            "Steps: 823\n",
            "Duration: 1.2869713306427002\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 35\n",
            "Steps: 883\n",
            "Duration: 1.3845398426055908\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 36\n",
            "Steps: 1005\n",
            "Duration: 1.5305438041687012\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 37\n",
            "Steps: 885\n",
            "Duration: 1.3526897430419922\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 38\n",
            "Steps: 1076\n",
            "Duration: 1.6871228218078613\n",
            "Score: -19.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 39\n",
            "Steps: 1171\n",
            "Duration: 1.8324780464172363\n",
            "Score: -18.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 40\n",
            "Steps: 823\n",
            "Duration: 1.2667934894561768\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 41\n",
            "Steps: 883\n",
            "Duration: 1.354457139968872\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 42\n",
            "Steps: 1041\n",
            "Duration: 1.6507148742675781\n",
            "Score: -20.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 43\n",
            "Steps: 943\n",
            "Duration: 1.458606243133545\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 44\n",
            "Steps: 887\n",
            "Duration: 1.3549377918243408\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 45\n",
            "Steps: 1041\n",
            "Duration: 1.610701560974121\n",
            "Score: -20.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 46\n",
            "Steps: 920\n",
            "Duration: 1.4442002773284912\n",
            "Score: -20.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 47\n",
            "Steps: 1076\n",
            "Duration: 1.6842155456542969\n",
            "Score: -19.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 48\n",
            "Steps: 1125\n",
            "Duration: 1.7558238506317139\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 49\n",
            "Steps: 1254\n",
            "Duration: 1.9579596519470215\n",
            "Score: -19.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Weights saved!\n",
            "\n",
            "Episode: 50\n",
            "Steps: 823\n",
            "Duration: 94.80840611457825\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9959589999999835\n",
            "\n",
            "Episode: 51\n",
            "Steps: 823\n",
            "Duration: 173.0456998348236\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9885609999999533\n",
            "\n",
            "Episode: 52\n",
            "Steps: 823\n",
            "Duration: 178.49530696868896\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9811629999999231\n",
            "\n",
            "Episode: 53\n",
            "Steps: 825\n",
            "Duration: 172.20506381988525\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9737469999998928\n",
            "\n",
            "Episode: 54\n",
            "Steps: 883\n",
            "Duration: 181.1472053527832\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9658089999998604\n",
            "\n",
            "Episode: 55\n",
            "Steps: 885\n",
            "Duration: 181.46340823173523\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9578529999998279\n",
            "\n",
            "Episode: 56\n",
            "Steps: 823\n",
            "Duration: 168.0433349609375\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9504549999997977\n",
            "\n",
            "Episode: 57\n",
            "Steps: 825\n",
            "Duration: 167.16659283638\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9430389999997674\n",
            "\n",
            "Episode: 58\n",
            "Steps: 763\n",
            "Duration: 156.79665637016296\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9361809999997394\n",
            "\n",
            "Episode: 59\n",
            "Steps: 1065\n",
            "Duration: 219.5090355873108\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9266049999997003\n",
            "\n",
            "Episode: 60\n",
            "Steps: 1124\n",
            "Duration: 228.22932195663452\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.916497999999659\n",
            "\n",
            "Target model updated\n",
            "\n",
            "Episode: 61\n",
            "Steps: 823\n",
            "Duration: 167.73796486854553\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.9090999999996288\n",
            "\n",
            "Episode: 62\n",
            "Steps: 920\n",
            "Duration: 186.1663420200348\n",
            "Score: -20.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.900828999999595\n",
            "\n",
            "Episode: 63\n",
            "Steps: 763\n",
            "Duration: 156.45941829681396\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.893970999999567\n",
            "\n",
            "Episode: 64\n",
            "Steps: 1069\n",
            "Duration: 219.3563995361328\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.8843589999995277\n",
            "\n",
            "Episode: 65\n",
            "Steps: 1039\n",
            "Duration: 212.91544771194458\n",
            "Score: -20.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.8750169999994896\n",
            "\n",
            "Episode: 66\n",
            "Steps: 945\n",
            "Duration: 194.21950101852417\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.8665209999994549\n",
            "\n",
            "Episode: 67\n",
            "Steps: 943\n",
            "Duration: 193.43902373313904\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.8580429999994202\n",
            "\n",
            "Episode: 68\n",
            "Steps: 763\n",
            "Duration: 157.50983500480652\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.8511849999993922\n",
            "\n",
            "Episode: 69\n",
            "Steps: 883\n",
            "Duration: 183.58096194267273\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.8432469999993598\n",
            "\n",
            "Episode: 70\n",
            "Steps: 945\n",
            "Duration: 194.37014937400818\n",
            "Score: -21.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 0.8347509999993251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-cfd86c2637af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtimee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set debug to true for rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-2bc039ed0b5a>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(name, env, agent, debug)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-2bc039ed0b5a>\u001b[0m in \u001b[0;36mtake_step\u001b[0;34m(name, env, agent, score, debug)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#9: If the threshold memory is satisfied, make the agent learn from memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarting_mem_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_frames_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-a7770083d196>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, debug)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}