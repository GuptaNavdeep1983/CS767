{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_second_attempt.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPxU5hcaRgVcmXAy38g6Rg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS767/blob/main/Assignment4_second_attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWMXWXeEm9vL"
      },
      "source": [
        "import random\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler \n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIukojRanEG3"
      },
      "source": [
        "iris = load_iris()\n",
        "\n",
        "iris_columns = ['sepal_len', 'sepal_width', 'petal_length', 'petal_width', 'target']\n",
        "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris_columns)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2VTt2ZmnBUo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# X = df[['sepal_len', 'sepal_width', 'petal_length', 'petal_width']]\n",
        "X = df[['sepal_width', 'petal_length']]\n",
        "\n",
        "scaler_training = StandardScaler() \n",
        "scaler_training.fit(X_train)\n",
        "scaled_values_training = scaler_training.transform(X)\n",
        "\n",
        "X = pd.DataFrame(scaled_values_training, columns=[*X.columns.values])\n",
        "X['target'] = df['target']\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7xVPtr6XuDim",
        "outputId": "96b5557e-2d71-45a7-f0fd-2a08bf253993"
      },
      "source": [
        "X"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.988604</td>\n",
              "      <td>-1.338748</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.167658</td>\n",
              "      <td>-1.338748</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.294847</td>\n",
              "      <td>-1.394978</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.063594</td>\n",
              "      <td>-1.282518</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.219857</td>\n",
              "      <td>-1.338748</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>-0.167658</td>\n",
              "      <td>0.798001</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>-1.323921</td>\n",
              "      <td>0.685540</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>-0.167658</td>\n",
              "      <td>0.798001</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>0.757352</td>\n",
              "      <td>0.910461</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>-0.167658</td>\n",
              "      <td>0.741771</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal_width  petal_length  target\n",
              "0       0.988604     -1.338748     0.0\n",
              "1      -0.167658     -1.338748     0.0\n",
              "2       0.294847     -1.394978     0.0\n",
              "3       0.063594     -1.282518     0.0\n",
              "4       1.219857     -1.338748     0.0\n",
              "..           ...           ...     ...\n",
              "145    -0.167658      0.798001     2.0\n",
              "146    -1.323921      0.685540     2.0\n",
              "147    -0.167658      0.798001     2.0\n",
              "148     0.757352      0.910461     2.0\n",
              "149    -0.167658      0.741771     2.0\n",
              "\n",
              "[150 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P-rkI38rAhm",
        "outputId": "05367abd-943d-4252-a270-27169b957783"
      },
      "source": [
        "from math import exp\n",
        "from random import seed\n",
        "from random import random\n",
        " \n",
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network\n",
        " \n",
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n",
        " \n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))\n",
        " \n",
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs\n",
        " \n",
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)\n",
        " \n",
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
        " \n",
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n",
        " \n",
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "  for epoch in range(n_epoch):\n",
        "    sum_error = 0\n",
        "    for row in train:\n",
        "      outputs = forward_propagate(network, row)\n",
        "      expected = [0 for i in range(n_outputs)]\n",
        "      expected[int(row[-1])] = 1\n",
        "      sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "      backward_propagate_error(network, expected)\n",
        "      update_weights(network, row, l_rate)\n",
        "    print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        " \n",
        "# Test training backprop algorithm\n",
        "seed(68)\n",
        "dataset = X.values\n",
        "n_inputs = len(dataset[0]) - 1\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "network = initialize_network(n_inputs, 3, n_outputs)\n",
        "print(network)\n",
        "train_network(network, dataset, 0.5, 50, n_outputs)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[{'weights': [0.7416689356145733, 0.9408038831472155, 0.7474033968541627]}, {'weights': [0.9081425629614371, 0.11097728168000565, 0.579773181999968]}, {'weights': [0.4318673124107113, 0.26549257874510634, 0.8604091679690017]}], [{'weights': [0.32940149633680194, 0.9736705910655336, 0.36125339691759495, 0.5536247242149994]}, {'weights': [0.7742260752576486, 0.7817480945876376, 0.041785415602753395, 0.7335413506775065]}, {'weights': [0.1060683270342535, 0.3424554918369298, 0.09338908488068065, 0.6053788824313064]}]]\n",
            ">epoch=0, lrate=0.500, error=61.624\n",
            ">epoch=1, lrate=0.500, error=57.338\n",
            ">epoch=2, lrate=0.500, error=44.932\n",
            ">epoch=3, lrate=0.500, error=40.375\n",
            ">epoch=4, lrate=0.500, error=38.943\n",
            ">epoch=5, lrate=0.500, error=38.019\n",
            ">epoch=6, lrate=0.500, error=37.276\n",
            ">epoch=7, lrate=0.500, error=36.302\n",
            ">epoch=8, lrate=0.500, error=34.707\n",
            ">epoch=9, lrate=0.500, error=32.487\n",
            ">epoch=10, lrate=0.500, error=29.898\n",
            ">epoch=11, lrate=0.500, error=27.184\n",
            ">epoch=12, lrate=0.500, error=24.517\n",
            ">epoch=13, lrate=0.500, error=22.015\n",
            ">epoch=14, lrate=0.500, error=19.758\n",
            ">epoch=15, lrate=0.500, error=17.795\n",
            ">epoch=16, lrate=0.500, error=16.156\n",
            ">epoch=17, lrate=0.500, error=14.837\n",
            ">epoch=18, lrate=0.500, error=13.802\n",
            ">epoch=19, lrate=0.500, error=12.996\n",
            ">epoch=20, lrate=0.500, error=12.366\n",
            ">epoch=21, lrate=0.500, error=11.869\n",
            ">epoch=22, lrate=0.500, error=11.473\n",
            ">epoch=23, lrate=0.500, error=11.152\n",
            ">epoch=24, lrate=0.500, error=10.890\n",
            ">epoch=25, lrate=0.500, error=10.672\n",
            ">epoch=26, lrate=0.500, error=10.491\n",
            ">epoch=27, lrate=0.500, error=10.338\n",
            ">epoch=28, lrate=0.500, error=10.208\n",
            ">epoch=29, lrate=0.500, error=10.098\n",
            ">epoch=30, lrate=0.500, error=10.003\n",
            ">epoch=31, lrate=0.500, error=9.923\n",
            ">epoch=32, lrate=0.500, error=9.853\n",
            ">epoch=33, lrate=0.500, error=9.794\n",
            ">epoch=34, lrate=0.500, error=9.744\n",
            ">epoch=35, lrate=0.500, error=9.701\n",
            ">epoch=36, lrate=0.500, error=9.664\n",
            ">epoch=37, lrate=0.500, error=9.634\n",
            ">epoch=38, lrate=0.500, error=9.609\n",
            ">epoch=39, lrate=0.500, error=9.588\n",
            ">epoch=40, lrate=0.500, error=9.572\n",
            ">epoch=41, lrate=0.500, error=9.559\n",
            ">epoch=42, lrate=0.500, error=9.549\n",
            ">epoch=43, lrate=0.500, error=9.542\n",
            ">epoch=44, lrate=0.500, error=9.538\n",
            ">epoch=45, lrate=0.500, error=9.536\n",
            ">epoch=46, lrate=0.500, error=9.536\n",
            ">epoch=47, lrate=0.500, error=9.538\n",
            ">epoch=48, lrate=0.500, error=9.541\n",
            ">epoch=49, lrate=0.500, error=9.546\n",
            "[{'weights': [-0.7033934018123543, 7.648706473354683, -4.292579541397398], 'output': 0.8169994805806113, 'delta': 0.003529795376153502}, {'weights': [1.4901432730183741, -4.877646472870028, -2.270431296120118], 'output': 0.002153753361760861, 'delta': 3.4089221224506614e-05}, {'weights': [0.4068717254221278, -5.013341233110685, 2.700142461557232], 'output': 0.25255490741466197, 'delta': -0.002267536016176925}]\n",
            "[{'weights': [-4.1482575832277995, 6.260966515981697, 0.36954732676128654, -3.3356905019179233], 'output': 0.0013343207784456135, 'delta': -1.778036299146251e-06}, {'weights': [-5.6922378430533325, -6.69396357366065, 2.3269523678705157, 1.2408241450117952], 'output': 0.05551716756639934, 'delta': -0.0029110433293296303}, {'weights': [5.826958705845112, -2.992731553283969, -4.337025959801122, -0.35216222359281163], 'output': 0.9646215197279538, 'delta': 0.0012073558561752915}]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}