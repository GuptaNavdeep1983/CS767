{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHDk9tq36nSU9HyRHjefnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS767/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA3_HENeBQVT"
      },
      "source": [
        "import random\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWw7ta9INmAc"
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "def sigmoid(x):\n",
        "  output = [0] * len(x)\n",
        "  for index, item in enumerate(x): \n",
        "    output[index] = 1.0 if item > 0.0 else 0.0 \n",
        "  return output\n",
        "def relu(x):\n",
        "  output = [0] * len(x)\n",
        "  for index, item in enumerate(x): \n",
        "    output[index] = item if item > 0.0 else item * 0.05\n",
        "  return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRmzSqyQUEeN"
      },
      "source": [
        "def categorical_cross_entropy(actual, predicted):\n",
        "\tsum_score = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\t\tsum_score += actual[i] * math.log(1e-15 + predicted[i])\n",
        "\tmean_sum_score = (1.0 / len(actual)) * sum_score\n",
        "\treturn -mean_sum_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFLe9USlBDxa"
      },
      "source": [
        "output_labels = [0.0, 1.0, 2.0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vkOKqUPFY0t"
      },
      "source": [
        "'''\n",
        "Class for the layer of the network \n",
        "which manages weights, bias, output and errors  \n",
        "'''\n",
        "class Layer():\n",
        "  def __init__(self, name, units, input_dim):\n",
        "    self.name = name\n",
        "    self.w = [[random.uniform(0,1) for _ in range(input_dim)] for _ in range(units)]\n",
        "    self.b = [0 for _ in range(units)]\n",
        "    self.output = [bias for bias in self.b]\n",
        "    self.next_layer = None\n",
        "    self.previous_layer = None\n",
        "    self.error = [0 for _ in range(units)]\n",
        "  def set_previous_layer(self, previous_layer):\n",
        "    self.previous_layer = previous_layer\n",
        "  def set_next_layer(self, next_layer):\n",
        "    self.next_layer = next_layer\n",
        "  '''\n",
        "  function to calculate the output of the neurons at current layer with \n",
        "  the given inputs\n",
        "  '''\n",
        "  def calculate_output(self, inputs, activation=sigmoid):\n",
        "    self.output = [bias for bias in self.b]\n",
        "    self.output += np.dot(self.w, inputs)\n",
        "    self.output = activation(self.output)\n",
        "    return self.output \n",
        "  '''\n",
        "  function to calculate the error in the output at current layer with \n",
        "  the given expected output\n",
        "  '''\n",
        "  def update_error(self, expected_output):\n",
        "    # last layer\n",
        "    if self.next_layer == None:\n",
        "      inputs = self.previous_layer.output\n",
        "      # total_error = sum([(exp_output-self.output[i])**2 for i, exp_output in enumerate(expected_output)])\n",
        "      # total_error = log_loss(expected_output, [self.output], labels=output_labels)\n",
        "      total_error = categorical_cross_entropy(self.output, expected_output)\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        self.error[row_index] = total_error * self.output[row_index] * (1-self.output[row_index]) \n",
        "    # layers other than last layer\n",
        "    else:\n",
        "      weights = self.next_layer.w\n",
        "      next_layer_error = self.next_layer.error\n",
        "      for output_idx, output in enumerate(self.output):\n",
        "        self.error[output_idx] = sum([weight[output_idx] * next_layer_error[weight_idx] * output * (1-output) for weight_idx, weight in enumerate(weights)])\n",
        "  '''\n",
        "  function to update the weights at current layer with \n",
        "  the given inputs\n",
        "  '''\n",
        "  def update_weights(self, learning_rate, row):\n",
        "    if self.next_layer == None:\n",
        "      inputs = self.previous_layer.output\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        for col_index, weight in enumerate(col):\n",
        "          self.w[row_index][col_index] = self.w[row_index][col_index] + (learning_rate * self.error[row_index] * inputs[col_index])\n",
        "          self.b[row_index] += learning_rate * self.error[row_index]\n",
        "    else:\n",
        "      inputs = row\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        for col_index, weight in enumerate(col):\n",
        "          self.w[row_index][col_index] = self.w[row_index][col_index] + (learning_rate * self.error[row_index] * inputs[col_index])\n",
        "          self.b[row_index] += learning_rate * self.error[row_index]\n",
        "  \n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLEqMj-uWIHC"
      },
      "source": [
        "# create network with the hidden and output layers\n",
        "def reset_layers(inputs, hidden_layer, output):\n",
        "  layer2 = Layer(\"Output\", output, hidden_layer)\n",
        "  layer1 = Layer(\"Hidden1\", hidden_layer, inputs)\n",
        "  layer2.set_previous_layer(layer1)\n",
        "  layer1.set_next_layer(layer2)\n",
        "  layers = [layer1, layer2]\n",
        "  return layers"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHMYuFAERXRP"
      },
      "source": [
        "# Load IRIS data\n",
        "iris = load_iris()\n",
        "\n",
        "iris_columns = ['sepal_len', 'sepal_width', 'petal_length', 'petal_width', 'target']\n",
        "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris_columns)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J359lR1SRnyu"
      },
      "source": [
        "# select desired columns, split into training and test datasets\n",
        "# standardize the inputs\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X = df[['sepal_len', 'sepal_width', 'petal_length', 'petal_width']]\n",
        "X = df[['sepal_width', 'petal_length']]\n",
        "y = df[['target']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state = 10)\n",
        "\n",
        "\n",
        "scaler_training = StandardScaler() \n",
        "scaler_training.fit(X_train)\n",
        "scaled_values_training = scaler_training.transform(X_train)\n",
        "scaled_values_testing = scaler_training.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(scaled_values_training, columns=[*X_train.columns.values])\n",
        "X_test = pd.DataFrame(scaled_values_testing, columns=[*X_test.columns.values])\n",
        "\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBdWTT0pSJU_"
      },
      "source": [
        "# set random seed\n",
        "import random\n",
        "random.seed(68)\n",
        "selected_rows = random.sample(range(0,119),   5)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmdWwns5U23f",
        "outputId": "ea53bcb9-2329-492d-c15b-12933c6746ac"
      },
      "source": [
        "# label binarizer\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit([0,1,2])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzwEWlmlRvLb",
        "outputId": "0189e0b1-6a8c-4e2c-fbcb-34382780b45c"
      },
      "source": [
        "def train_weights(X_train, y_train, num_epochs, learning_rate, layers):\n",
        "  index = 0;\n",
        "  # select mini batch\n",
        "  mini_batch = X_train.loc[selected_rows, :]\n",
        "  y_train = y_train.loc[selected_rows, :]\n",
        "  while index < num_epochs:\n",
        "    sum_error = 0\n",
        "    print(\"============================================\")\n",
        "    print(\"=================Iteration %d===============\"%(index+1))\n",
        "    print(\"============================================\")\n",
        "    for rowIndex, row in mini_batch.iterrows():\n",
        "      # pass the inputs and get the outputs in a typical feed forward network.\n",
        "      output = row\n",
        "      for layer_idx, layer in enumerate(layers):\n",
        "        if layer_idx == len(layers) - 1:\n",
        "          output = layer.calculate_output(output, softmax)\n",
        "        else:\n",
        "          output = layer.calculate_output(output, relu)\n",
        "      # calculate total error \n",
        "      expected_output = lb.transform([y_train.loc[rowIndex,'target']])[0]\n",
        "      sum_error += sum([(exp_output-output[i])**2 for i, exp_output in enumerate(expected_output)])\n",
        "\n",
        "      # update error at each layer\n",
        "      for layer_idx, layer in enumerate(reversed(layers)):\n",
        "        # layer.update_error([y_train.loc[rowIndex,'target']])\n",
        "        layer.update_error(expected_output)\n",
        "      print(\"error of output layer\", layers[1].error)\n",
        "      print(\"error of hidden layer\", layers[0].error)  \n",
        "      # update weights at each layer\n",
        "      for layer_idx, layer in enumerate(reversed(layers)):\n",
        "        layer.update_weights(learning_rate, row)\n",
        "      \n",
        "      # error = log_loss([y_train.loc[rowIndex,'target']], [output], labels=[0.0, 1.0, 2.0])\n",
        "    print(\"Entropy based loss: %f\"% (sum_error))\n",
        "      # if error != 0:\n",
        "      #   layer1_error = layers[1].update_weights(error, layers[0].output, learning_rate)\n",
        "      #   layers[0].update_weights(layer1_error, row, learning_rate)\n",
        "    print(\"Weights of output layer\", layers[1].w)\n",
        "    print(\"Weights of hidden layer\", layers[0].w)\n",
        "    index = index + 1\n",
        "\n",
        "layers = reset_layers(2, 3, 3)\n",
        "print(\"Creating model using training data\")\n",
        "train_weights(X_train, y_train, 2, 0.5, layers)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating model using training data\n",
            "============================================\n",
            "=================Iteration 1===============\n",
            "============================================\n",
            "error of output layer [1.8754086013465177, 1.7325333993731837, 1.6967238030474185]\n",
            "error of hidden layer [0.6856222888517162, -0.012493294561555393, -0.009954467320011394]\n",
            "error of output layer [2.2756176314800016, 1.6497474488384418, 1.4005201506001135]\n",
            "error of hidden layer [-3.4923703141141234, -0.02939211258730893, -0.03218172820769665]\n",
            "error of output layer [2.314178477136166, 1.7477016564433796, 1.3316065430229087]\n",
            "error of hidden layer [-4.610978256062003, -0.167816876725929, -0.25876285207105754]\n",
            "error of output layer [0.3190722804706468, 0.2472252961268001, 0.10024995213060675]\n",
            "error of hidden layer [-0.41361074808804504, -0.0021621519762701903, -0.009002232890080663]\n",
            "error of output layer [1.6159827265233397, 1.3057506109174999, 0.46992129041574987]\n",
            "error of hidden layer [-7.100160394963258, -0.027906772416722545, -0.09505659564295987]\n",
            "Entropy based loss: 4.281759\n",
            "Weights of output layer [[2.044867447521977, -0.039964511860138933, 0.4384891869266227], [1.2674451066898447, 0.14932183507464328, 0.7514801260027195], [1.3129376334730385, 0.8947279411549828, 0.2903203858685378]]\n",
            "Weights of hidden layer [[8.116258523036137, -3.7195213186599627], [0.966932190154583, -0.03914748570558983], [1.0325499432438332, -0.017047195155620237]]\n",
            "============================================\n",
            "=================Iteration 2===============\n",
            "============================================\n",
            "error of output layer [1.3880456523941305, 1.3045326383727776, 0.1138379898841557]\n",
            "error of hidden layer [-8.21402901649299, -0.005225360187731905, -0.049075968480737656]\n",
            "error of output layer [1.7130970356766342, 1.7064309035421015, 0.010459793365570184]\n",
            "error of hidden layer [-15.46036124219451, -0.005103665934542192, -0.09252152724976881]\n",
            "error of output layer [2.866409577609452, 2.86640930760049, 4.2132386944218696e-06]\n",
            "error of hidden layer [35.18003005523178, -0.008294771757361354, -0.4772067503691798]\n",
            "error of output layer [0.0033528764792688293, 1.771166484960153e-05, 0.0033705778218520825]\n",
            "error of hidden layer [0.11018247253324523, -3.9361373199227575e-05, -7.658341810520757e-05]\n",
            "error of output layer [2.5970998214617365e-23, 6.324417763107447e-28, 0.0]\n",
            "error of hidden layer [1.376913030352746e-20, 2.958212092340204e-25, -5.552658051799007e-25]\n",
            "Entropy based loss: 8.934439\n",
            "Weights of output layer [[-5.168699618485052, -0.24358231672404623, 0.18391897475533203], [-5.907894925399491, -0.053268575183680486, 0.49836355462135906], [1.2576791653271322, 0.8933145491003142, 0.2883347069767251]]\n",
            "Weights of hidden layer [[-23.611741926314867, -2.004920650382337], [0.9767595696983657, -0.04606861795739702], [1.5365490779143807, -0.2419450626303295]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFbrih6qplAA"
      },
      "source": [
        "# select desired columns, split into training and test datasets\n",
        "# standardize the inputs\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df[['sepal_len', 'sepal_width', 'petal_length', 'petal_width']]\n",
        "y = df[['target']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state = 10)\n",
        "\n",
        "\n",
        "scaler_training = StandardScaler() \n",
        "scaler_training.fit(X_train)\n",
        "scaled_values_training = scaler_training.transform(X_train)\n",
        "scaled_values_testing = scaler_training.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(scaled_values_training, columns=[*X_train.columns.values])\n",
        "X_test = pd.DataFrame(scaled_values_testing, columns=[*X_test.columns.values])\n",
        "\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHyOwDDVpDBu",
        "outputId": "537d162b-2b91-4c99-b57c-2331eb0dcc74"
      },
      "source": [
        "layers = reset_layers(4, 5, 3)\n",
        "train_weights(X_train, y_train, 2, 0.5, layers)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================\n",
            "=================Iteration 1===============\n",
            "============================================\n",
            "error of output layer [1.3883555655755018, 1.7167845311626557, 1.693694629492002]\n",
            "error of hidden layer [0.3558778605162929, 0.8947744233284439, 0.567037412652011, -0.249267636271698, 0.35613520575305235]\n",
            "error of output layer [0.31886539237147316, 1.2486251351941033, 1.301064636369684]\n",
            "error of hidden layer [-0.8676264394919198, -17.730124931041296, -6.966835143813515, -0.18406737032136983, -0.6991921117608071]\n",
            "error of output layer [1.66655991725264, 1.199625258894805, 0.6417351991456913]\n",
            "error of hidden layer [-0.7314387798995976, -69.87555237656701, -11.084711989581113, -0.13667740375813575, -0.5107176212454484]\n",
            "error of output layer [1.2431151005560965e-07, 1.238354444298128e-07, 4.761642098664742e-10]\n",
            "error of hidden layer [-4.004215221574913e-08, -2.56040381980099e-06, -3.062077523862442e-07, -1.1164835061173562e-07, -2.8132275426099095e-08]\n",
            "error of output layer [8.806739388136755e-12, 8.807400508123762e-12, 1.559789504775641e-17]\n",
            "error of hidden layer [-4.698987340481697e-12, -1.1874833112005985e-09, -1.2536885036073163e-10, -6.616292095121728e-13, -2.9268996235816063e-12]\n",
            "Entropy based loss: 4.522277\n",
            "Weights of output layer [[1.2825556381063887, -0.6734344075164823, 0.24033140209258363, 1.2755419922547688, 0.47662214607059206], [1.175335313257403, 1.4759206961694553, 1.4082076713456946, 2.48951018724725, 1.7956348829154463], [1.3302514889981676, 2.2479722636700745, 2.355200225624105, 2.3124677700517338, 1.178236035350584]]\n",
            "Weights of hidden layer [[-0.11007567950172896, 1.6501678526490846, -0.0781285099086103, -0.06465499855990893], [-9.75944009034212, 74.3968103962772, -32.684624220580865, -18.4362759696843], [-2.571993943668661, 12.98170612092927, -6.3857204090806405, -4.356802802577913], [0.6088892742485549, 0.2864400946513277, 0.6862152395176955, 0.42558695943126457], [0.05003788914649207, 1.3855953344955998, 0.06613473120657083, -0.21137028078785067]]\n",
            "============================================\n",
            "=================Iteration 2===============\n",
            "============================================\n",
            "error of output layer [2.8869999979594467e-10, 2.8869820697487884e-10, 2.0057252336282202e-15]\n",
            "error of hidden layer [-1.1660365935033859e-10, -3.0590396252057535e-08, -3.4134612980933627e-09, -2.1024256028161028e-11, -7.426927473770076e-11]\n",
            "error of output layer [4.806000014431209e-12, 4.805059196267283e-12, 6.091768052114072e-18]\n",
            "error of hidden layer [-2.2938771940534663e-12, -6.781127207454002e-10, -7.239402990551506e-11, 1.3435915823206622e-12, -1.4176936422124613e-12]\n",
            "error of output layer [0.0, 3.4283345934669124e-17, 4.780581729810852e-25]\n",
            "error of hidden layer [-1.5425879282793942e-17, -1.676438270853135e-14, -7.133869608745212e-16, -4.470425312359733e-18, -1.708937138932593e-17]\n",
            "error of output layer [1.243110592254567e-07, 1.238349964280417e-07, 4.761613805891401e-10]\n",
            "error of hidden layer [-4.00420112386719e-08, -2.560393475395999e-06, -3.062066708952136e-07, -1.1164796676825018e-07, -2.8132176124739286e-08]\n",
            "error of output layer [8.806739388136755e-12, 8.807398338277715e-12, 1.5597827744677434e-17]\n",
            "error of hidden layer [-4.698986803057928e-12, -1.1874819451443556e-09, -1.2536881720649327e-10, -6.616292392533542e-13, -2.926899065445651e-12]\n",
            "Entropy based loss: 8.000000\n",
            "Weights of output layer [[1.2825556307955612, -0.6734346939566624, 0.2403313508220177, 1.2755419798837775, 0.4766221403768273], [1.1753353059744893, 1.4759204108198314, 1.408207620270182, 2.4895101749236246, 1.7956348772434265], [1.330251488970248, 2.2479722625792933, 2.3552002254290114, 2.312467770004358, 1.1782360353288346]]\n",
            "Weights of hidden layer [[-0.11007564200421213, 1.6501678560174808, -0.07812847976383461, -0.06465496959903437], [-9.75943769491461, 74.3968106139863, -32.68462230014819, -18.43627412692696], [-2.571993657163373, 12.981706146938368, -6.38572017933071, -4.35680258209732], [0.6088893788320315, 0.28644010401237185, 0.6862153236634502, 0.4255870403017852], [0.0500379154917757, 1.3855953368613236, 0.06613475238765887, -0.21137026043788543]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}