{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDud3RJxkoblgeAMUQLMo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS767/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA3_HENeBQVT"
      },
      "source": [
        "import random\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler "
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWw7ta9INmAc"
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "def sigmoid(x):\n",
        "  output = [0] * len(x)\n",
        "  for index, item in enumerate(x): \n",
        "    output[index] = 1.0 if item > 0.0 else 0.0 \n",
        "  return output\n",
        "def relu(x):\n",
        "  output = [0] * len(x)\n",
        "  for index, item in enumerate(x): \n",
        "    output[index] = item if item > 0.0 else 0.0 \n",
        "  return output"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vkOKqUPFY0t"
      },
      "source": [
        "class Layer():\n",
        "  def __init__(self, name, units, input_dim):\n",
        "    self.name = name\n",
        "    self.w = [[random.uniform(0,1) for _ in range(input_dim)] for _ in range(units)]\n",
        "    self.b = [0 for _ in range(units)]\n",
        "    self.output = [bias for bias in self.b]\n",
        "  def calculate_output(self, inputs, activation=sigmoid):\n",
        "    self.output = [bias for bias in self.b]\n",
        "    self.output += np.dot(self.w, inputs)\n",
        "    self.output = activation(self.output)\n",
        "    return self.output \n",
        "  def update_weights(self, error, inputs, learning_rate):\n",
        "    error_out = 0\n",
        "    for input_idx, input in enumerate(inputs):\n",
        "      error_out += sum([learning_rate * error * weight[input_idx] for weight_idx, weight in enumerate(self.w)])\n",
        "    for row_index, col in enumerate(self.w):\n",
        "      for col_index, weight in enumerate(col):\n",
        "        self.w[row_index][col_index] = self.w[row_index][col_index] - (learning_rate * error * inputs[col_index])\n",
        "        self.b[row_index] = self.b[row_index] - (learning_rate * error)\n",
        "    return error_out"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLEqMj-uWIHC"
      },
      "source": [
        "def reset_layers():\n",
        "  layer1 = Layer(\"Hidden1\", 3, 2)\n",
        "  layer2 = Layer(\"Output\", 3, 3)\n",
        "  layers = [layer1, layer2]\n",
        "  return layers"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHMYuFAERXRP"
      },
      "source": [
        "iris = load_iris()\n",
        "\n",
        "iris_columns = ['sepal_len', 'sepal_width', 'petal_length', 'petal_width', 'target']\n",
        "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris_columns)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J359lR1SRnyu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# X = df[['sepal_len', 'sepal_width', 'petal_length', 'petal_width']]\n",
        "X = df[['sepal_width', 'petal_length']]\n",
        "y = df[['target']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state = 10)\n",
        "\n",
        "\n",
        "scaler_training = StandardScaler() \n",
        "scaler_training.fit(X_train)\n",
        "scaled_values_training = scaler_training.transform(X_train)\n",
        "scaled_values_testing = scaler_training.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(scaled_values_training, columns=[*X_train.columns.values])\n",
        "X_test = pd.DataFrame(scaled_values_testing, columns=[*X_test.columns.values])\n",
        "\n",
        "# X_train['bias'] = 1\n",
        "# X_test['bias'] = 1\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "0_K4TLplGfp6",
        "outputId": "78f02f3b-40c9-4bca-cbd9-d1f8d9d47afb"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.398911</td>\n",
              "      <td>0.460619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.398911</td>\n",
              "      <td>0.291929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.167658</td>\n",
              "      <td>1.135382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.630163</td>\n",
              "      <td>0.741771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.323921</td>\n",
              "      <td>0.685540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>-1.323921</td>\n",
              "      <td>0.685540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>-0.398911</td>\n",
              "      <td>-0.101683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>3.069877</td>\n",
              "      <td>-1.282518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>0.294847</td>\n",
              "      <td>1.247843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>0.063594</td>\n",
              "      <td>-1.282518</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal_width  petal_length\n",
              "0      -0.398911      0.460619\n",
              "1      -0.398911      0.291929\n",
              "2      -0.167658      1.135382\n",
              "3      -0.630163      0.741771\n",
              "4      -1.323921      0.685540\n",
              "..           ...           ...\n",
              "115    -1.323921      0.685540\n",
              "116    -0.398911     -0.101683\n",
              "117     3.069877     -1.282518\n",
              "118     0.294847      1.247843\n",
              "119     0.063594     -1.282518\n",
              "\n",
              "[120 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBdWTT0pSJU_"
      },
      "source": [
        "import random\n",
        "random.seed(34)\n",
        "selected_rows = random.sample(range(0,119),   5)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmdWwns5U23f",
        "outputId": "224dfb1c-edfe-4eb3-fe6d-9a6cfcec5ce7"
      },
      "source": [
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit([0,1,2])\n",
        "lb.transform([0])\n"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRmzSqyQUEeN"
      },
      "source": [
        "def categorical_cross_entropy(actual, predicted):\n",
        "\tsum_score = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\t\tsum_score += actual[i] * math.log(1e-15 + predicted[i])\n",
        "\tmean_sum_score = (1.0 / len(actual)) * sum_score\n",
        "\treturn -mean_sum_score"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzwEWlmlRvLb",
        "outputId": "218134ea-5e3a-4660-d96b-89648efa99b1"
      },
      "source": [
        "def train_weights_using_mini_batch_gradient_descent(X_train, y_train, num_epochs, learning_rate, layers):\n",
        "  # weights for all the features \n",
        "  index = 0;\n",
        "  error = 0\n",
        "  mini_batch = X_train.loc[selected_rows, :]\n",
        "  y_train = y_train.loc[selected_rows, :]\n",
        "  while index < num_epochs:\n",
        "    print(\"============================================\")\n",
        "    print(\"=================Iteration %d===============\"%(index+1))\n",
        "    print(\"============================================\")\n",
        "    for rowIndex, row in mini_batch.iterrows():\n",
        "      output = row\n",
        "      for layer_idx, layer in enumerate(layers):\n",
        "        if layer_idx == len(layers) - 1:\n",
        "          output = layer.calculate_output(output, softmax)\n",
        "        else:\n",
        "          output = layer.calculate_output(output, sigmoid)\n",
        "      error = log_loss([y_train.loc[rowIndex,'target']], [output], labels=[0.0, 1.0, 2.0])\n",
        "      print(\"Entropy based loss: %f\"% (error))\n",
        "      if error != 0:\n",
        "        layer1_error = layers[1].update_weights(error, layers[0].output, learning_rate)\n",
        "        layers[0].update_weights(layer1_error, row, learning_rate)\n",
        "      print(\"New Weights of output layer\", layers[1].w)\n",
        "      print(\"New Weights of hidden layer\", layers[0].w)\n",
        "    # for layer_idx, layer in enumerate(layers):\n",
        "    #   layer.update_weights()\n",
        "    index = index + 1\n",
        "  # df_plot = pd.DataFrame(df_plot, columns=['mse', 'epoch'])\n",
        "  return\n",
        "\n",
        "layers = reset_layers()\n",
        "print(\"Creating model using training data\")\n",
        "train_weights_using_mini_batch_gradient_descent(X_train, y_train, 10, 0.5, layers)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating model using training data\n",
            "============================================\n",
            "=================Iteration 1===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[2.6168186835167146, 0.5356655828156272], [3.166451123351512, 0.3627941862695465], [2.3187479037967482, 0.9346497520338976]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.5192862747721043, -0.5266369736696637], [4.068918714606902, -0.6995083702157443], [3.221215495052138, -0.12765280445139326]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[2.4346692614284704, 1.2295511440321742], [2.9843017012632678, 1.0566797474860936], [2.136598481708504, 1.6285353132504445]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.33713685268386, 0.16724858754688343], [3.8867692925186574, -0.005622808999197204], [3.0390660729638936, 0.5662327567651537]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[0.5966160021743736, 2.1650216201790466], [1.146248442009171, 1.992150223632966], [0.2985452224544072, 2.564005789397317]]\n",
            "============================================\n",
            "=================Iteration 2===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[2.823806663162445, 2.1495870283918324], [3.3734391029972426, 1.9767156318457515], [2.525735883442479, 2.548571197610103]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.726274254417835, 1.0872844719065415], [4.275906694252632, 0.9144130753604607], [3.4282034746978685, 1.486268641124812]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[2.641657241074201, 2.8434725896083792], [3.1912896809089983, 2.6706011930622986], [2.3435864613542345, 3.24245675882665]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.5441248323295906, 1.7811700331230884], [4.093757272164388, 1.6082986365770078], [3.246054052609624, 2.1801542023413596]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[0.8036039818201042, 3.778943065755252], [1.3532364216549015, 3.6060716692091708], [0.5055332021001377, 4.177927234973523]]\n",
            "============================================\n",
            "=================Iteration 3===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.030794642808176, 3.7635084739680376], [3.580427082642973, 3.5906370774219565], [2.7327238630882094, 4.162492643186308]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.9332622340635655, 2.7012059174827465], [4.482894673898363, 2.5283345209366654], [3.635191454343599, 3.1001900867010175]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[2.8486452207199315, 4.457394035184585], [3.398277660554729, 4.284522638638503], [2.550574440999965, 4.856378204402855]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.751112811975321, 3.3950914786992943], [4.300745251810119, 3.2222200821532123], [3.4530420322553548, 3.7940756479175644]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[1.0105919614658347, 5.3928645113314575], [1.5602244013006321, 5.2199931147853755], [0.7125211817458683, 5.791848680549728]]\n",
            "============================================\n",
            "=================Iteration 4===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.2377826224539064, 5.377429919544243], [3.7874150622887037, 5.204558522998161], [2.93971184273394, 5.776414088762513]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.140250213709296, 4.315127363058952], [4.689882653544093, 4.14225596651287], [3.8421794339893296, 4.714111532277222]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.0556332003656617, 6.07131548076079], [3.6052656402004595, 5.898444084214708], [2.7575624206456957, 6.47029964997906]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.9581007916210513, 5.009012924275499], [4.507733231455849, 4.836141527729417], [3.6600300119010853, 5.407997093493769]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[1.2175799411115649, 7.006785956907662], [1.7672123809463627, 6.83391456036158], [0.9195091613915989, 7.405770126125932]]\n",
            "============================================\n",
            "=================Iteration 5===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.4447706020996365, 6.9913513651204475], [3.9944030419344343, 6.818479968574366], [3.1466998223796705, 7.390335534338718]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.347238193355026, 5.929048808635157], [4.896870633189824, 5.756177412089075], [4.04916741363506, 6.328032977853427]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.2626211800113922, 7.685236926336994], [3.81225361984619, 7.512365529790912], [2.9645504002914262, 8.084221095555264]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.165088771266782, 6.622934369851704], [4.71472121110158, 6.450062973305622], [3.867017991546816, 7.021918539069974]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[1.4245679207572954, 8.620707402483866], [1.9742003605920933, 8.447836005937784], [1.1264971410373295, 9.019691571702136]]\n",
            "============================================\n",
            "=================Iteration 6===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.651758581745367, 8.605272810696652], [4.201391021580164, 8.43240141415057], [3.353687802025401, 9.004256979914922]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.554226173000757, 7.542970254211362], [5.103858612835554, 7.37009885766528], [4.256155393280791, 7.941954423429632]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.469609159657123, 9.2991583719132], [4.01924159949192, 9.126286975367117], [3.171538379937157, 9.69814254113147]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.3720767509125125, 8.236855815427909], [4.921709190747309, 8.063984418881827], [4.0740059711925465, 8.635839984646179]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[1.631555900403026, 10.234628848060073], [2.181188340237823, 10.06175745151399], [1.33348512068306, 10.633613017278343]]\n",
            "============================================\n",
            "=================Iteration 7===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.8587465613910976, 10.219194256272859], [4.408379001225894, 10.046322859726777], [3.5606757816711316, 10.618178425491129]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.761214152646487, 9.156891699787568], [5.310846592481283, 8.984020303241486], [4.463143372926521, 9.555875869005838]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.6765971393028534, 10.913079817489406], [4.22622957913765, 10.740208420943324], [3.3785263595828874, 11.312063986707676]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.579064730558243, 9.850777261004115], [5.128697170393039, 9.677905864458033], [4.280993950838277, 10.249761430222385]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[1.8385438800487566, 11.84855029363628], [2.3881763198835526, 11.675678897090197], [1.5404731003287906, 12.24753446285455]]\n",
            "============================================\n",
            "=================Iteration 8===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.065734541036829, 11.833115701849065], [4.615366980871624, 11.660244305302983], [3.767663761316862, 12.232099871067335]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.968202132292218, 10.770813145363775], [5.517834572127013, 10.597941748817693], [4.670131352572252, 11.169797314582045]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[3.883585118948584, 12.527001263065612], [4.4332175587833795, 12.35412986651953], [3.585514339228618, 12.925985432283882]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.786052710203974, 11.464698706580322], [5.335685150038769, 11.29182731003424], [4.487981930484008, 11.863682875798592]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[2.045531859694487, 13.462471739212486], [2.5951642995292823, 13.289600342666404], [1.7474610799745212, 13.861455908430756]]\n",
            "============================================\n",
            "=================Iteration 9===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.272722520682558, 13.447037147425272], [4.8223549605173535, 13.27416575087919], [3.9746517409625928, 13.846021316643542]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[5.1751901119379475, 12.384734590939981], [5.724822551772743, 12.2118631943939], [4.877119332217982, 12.783718760158251]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.090573098594314, 14.140922708641819], [4.640205538429109, 13.968051312095737], [3.7925023188743485, 14.539906877860089]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.993040689849703, 13.078620152156528], [5.542673129684498, 12.905748755610446], [4.694969910129738, 13.477604321374798]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[2.252519839340217, 15.076393184788692], [2.802152279175012, 14.90352178824261], [1.9544490596202517, 15.475377354006962]]\n",
            "============================================\n",
            "=================Iteration 10===============\n",
            "============================================\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.479710500328288, 15.060958593001478], [5.029342940163083, 14.888087196455396], [4.181639720608324, 15.459942762219749]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[5.382178091583677, 13.998656036516188], [5.931810531418472, 13.825784639970106], [5.084107311863713, 14.397640205734458]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[4.297561078240044, 15.754844154218025], [4.847193518074839, 15.581972757671943], [3.999490298520079, 16.153828323436297]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[5.200028669495433, 14.692541597732735], [5.749661109330228, 14.519670201186653], [4.901957889775469, 15.091525766951007]]\n",
            "Entropy based loss: 1.098612\n",
            "New Weights of output layer [[0.6782766303016691, 0.7977875836019059, 0.3729203719561375], [0.37241676070606033, 0.7983418887253494, 0.6052236050327029], [0.4009222186862581, 0.420449824347251, 0.7679392435730371]]\n",
            "New Weights of hidden layer [[2.4595078189859465, 16.6903146303649], [3.0091402588207417, 16.517443233818817], [2.1614370392659823, 17.08929879958317]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS3EIJKe6PyB",
        "outputId": "265c8249-c666-404a-f76f-d48ff08addd8"
      },
      "source": [
        "for rowIndex, row in X_test.iterrows():\n",
        "  # print(rowIndex)\n",
        "  output = row\n",
        "  for layer_idx, layer in enumerate(layers):\n",
        "      if layer_idx == len(layers) - 1:\n",
        "        output = layer.calculate_output(output, softmax)\n",
        "      else:\n",
        "        output = layer.calculate_output(output, relu)\n",
        "  print(output)\n",
        "  print(\"Predicted: %d, Actual %d\"%(np.argmax(output), y_test.loc[rowIndex, 'target']))"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 2\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 2\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 2\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 2\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 2\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 2\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 0\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 1\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "Predicted: 0, Actual 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}