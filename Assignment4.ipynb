{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNP7D0QIn9ou349hv1yXHRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS767/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA3_HENeBQVT"
      },
      "source": [
        "import random\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler "
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWw7ta9INmAc"
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "def sigmoid(x):\n",
        "  output = [0] * len(x)\n",
        "  for index, item in enumerate(x): \n",
        "    output[index] = 1.0 if item > 0.0 else 0.0 \n",
        "  return output\n",
        "def relu(x):\n",
        "  output = [0] * len(x)\n",
        "  for index, item in enumerate(x): \n",
        "    output[index] = item if item > 0.0 else item * 0.05\n",
        "  return output"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFLe9USlBDxa"
      },
      "source": [
        "output_labels = [0.0, 1.0, 2.0]"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vkOKqUPFY0t"
      },
      "source": [
        "class Layer():\n",
        "  def __init__(self, name, units, input_dim):\n",
        "    self.name = name\n",
        "    self.w = [[random.uniform(0,1) for _ in range(input_dim)] for _ in range(units)]\n",
        "    self.b = [0 for _ in range(units)]\n",
        "    self.output = [bias for bias in self.b]\n",
        "    self.next_layer = None\n",
        "    self.previous_layer = None\n",
        "    self.error = [0 for _ in range(units)]\n",
        "  def set_previous_layer(self, previous_layer):\n",
        "    self.previous_layer = previous_layer\n",
        "  def set_next_layer(self, next_layer):\n",
        "    self.next_layer = next_layer\n",
        "  def calculate_output(self, inputs, activation=sigmoid):\n",
        "    self.output = [bias for bias in self.b]\n",
        "    self.output += np.dot(self.w, inputs)\n",
        "    self.output = activation(self.output)\n",
        "    return self.output \n",
        "  def update_error(self, expected_output):\n",
        "    if self.next_layer == None:\n",
        "      inputs = self.previous_layer.output\n",
        "      # total_error = sum([(exp_output-self.output[i])**2 for i, exp_output in enumerate(expected_output)])\n",
        "      # total_error = log_loss(expected_output, [self.output], labels=output_labels)\n",
        "      total_error = categorical_cross_entropy(self.output, expected_output)\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        self.error[row_index] = total_error * self.output[row_index] * (1-self.output[row_index]) \n",
        "    else:\n",
        "      weights = self.next_layer.w\n",
        "      next_layer_error = self.next_layer.error\n",
        "      for output_idx, output in enumerate(self.output):\n",
        "        print(output)\n",
        "        self.error[output_idx] = sum([weight[output_idx] * next_layer_error[weight_idx] * output * (1-output) for weight_idx, weight in enumerate(weights)])\n",
        "  def update_weights(self, learning_rate, row):\n",
        "    if self.next_layer == None:\n",
        "      inputs = self.previous_layer.output\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        for col_index, weight in enumerate(col):\n",
        "          self.w[row_index][col_index] = self.w[row_index][col_index] + (learning_rate * self.error[row_index] * inputs[col_index])\n",
        "          self.b[row_index] += learning_rate * self.error[row_index]\n",
        "    else:\n",
        "      inputs = row\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        for col_index, weight in enumerate(col):\n",
        "          self.w[row_index][col_index] = self.w[row_index][col_index] + (learning_rate * self.error[row_index] * inputs[col_index])\n",
        "          self.b[row_index] += learning_rate * self.error[row_index]\n",
        "  \n",
        "    "
      ],
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLEqMj-uWIHC"
      },
      "source": [
        "def reset_layers():\n",
        "  layer2 = Layer(\"Output\", 3, 3)\n",
        "  layer1 = Layer(\"Hidden1\", 3, 2)\n",
        "  layer2.set_previous_layer(layer1)\n",
        "  layer1.set_next_layer(layer2)\n",
        "  layers = [layer1, layer2]\n",
        "  return layers"
      ],
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHMYuFAERXRP"
      },
      "source": [
        "iris = load_iris()\n",
        "\n",
        "iris_columns = ['sepal_len', 'sepal_width', 'petal_length', 'petal_width', 'target']\n",
        "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris_columns)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J359lR1SRnyu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# X = df[['sepal_len', 'sepal_width', 'petal_length', 'petal_width']]\n",
        "X = df[['sepal_width', 'petal_length']]\n",
        "y = df[['target']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state = 10)\n",
        "\n",
        "\n",
        "# scaler_training = StandardScaler() \n",
        "# scaler_training.fit(X_train)\n",
        "# scaled_values_training = scaler_training.transform(X_train)\n",
        "# scaled_values_testing = scaler_training.transform(X_test)\n",
        "\n",
        "# X_train = pd.DataFrame(scaled_values_training, columns=[*X_train.columns.values])\n",
        "# X_test = pd.DataFrame(scaled_values_testing, columns=[*X_test.columns.values])\n",
        "\n",
        "# X_train['bias'] = 1\n",
        "# X_test['bias'] = 1\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "0_K4TLplGfp6",
        "outputId": "49f857e4-8b1f-433f-b7b3-48b6319f677c"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.9</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.9</td>\n",
              "      <td>4.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>5.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.8</td>\n",
              "      <td>5.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>2.9</td>\n",
              "      <td>3.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>4.4</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>3.2</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal_width  petal_length\n",
              "0            2.9           4.6\n",
              "1            2.9           4.3\n",
              "2            3.0           5.8\n",
              "3            2.8           5.1\n",
              "4            2.5           5.0\n",
              "..           ...           ...\n",
              "115          2.5           5.0\n",
              "116          2.9           3.6\n",
              "117          4.4           1.5\n",
              "118          3.2           6.0\n",
              "119          3.1           1.5\n",
              "\n",
              "[120 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBdWTT0pSJU_"
      },
      "source": [
        "import random\n",
        "random.seed(34)\n",
        "selected_rows = random.sample(range(0,119),   10)"
      ],
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmdWwns5U23f",
        "outputId": "224dfb1c-edfe-4eb3-fe6d-9a6cfcec5ce7"
      },
      "source": [
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit([0,1,2])\n",
        "lb.transform([0])\n"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRmzSqyQUEeN"
      },
      "source": [
        "def categorical_cross_entropy(actual, predicted):\n",
        "\tsum_score = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\t\tsum_score += actual[i] * math.log(1e-15 + predicted[i])\n",
        "\tmean_sum_score = (1.0 / len(actual)) * sum_score\n",
        "\treturn -mean_sum_score"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzwEWlmlRvLb",
        "outputId": "b3a0de60-8b26-4164-aefb-322b1377bdde"
      },
      "source": [
        "def train_weights_using_mini_batch_gradient_descent(X_train, y_train, num_epochs, learning_rate, layers):\n",
        "  # weights for all the features \n",
        "  index = 0;\n",
        "  \n",
        "  mini_batch = X_train.loc[selected_rows, :]\n",
        "  y_train = y_train.loc[selected_rows, :]\n",
        "  while index < num_epochs:\n",
        "    sum_error = 0\n",
        "    print(\"============================================\")\n",
        "    print(\"=================Iteration %d===============\"%(index+1))\n",
        "    print(\"============================================\")\n",
        "    for rowIndex, row in mini_batch.iterrows():\n",
        "      # print(row)\n",
        "      output = row\n",
        "      for layer_idx, layer in enumerate(layers):\n",
        "        if layer_idx == len(layers) - 1:\n",
        "          output = layer.calculate_output(output, softmax)\n",
        "        else:\n",
        "          output = layer.calculate_output(output, relu)\n",
        "\n",
        "      expected_output = lb.transform([y_train.loc[rowIndex,'target']])[0]\n",
        "      sum_error += sum([(exp_output-output[i])**2 for i, exp_output in enumerate(expected_output)])\n",
        "      for layer_idx, layer in enumerate(reversed(layers)):\n",
        "        # layer.update_error([y_train.loc[rowIndex,'target']])\n",
        "        layer.update_error(expected_output)\n",
        "      print(\"error of output layer\", layers[1].error)\n",
        "      print(\"error of hidden layer\", layers[0].error)  \n",
        "      for layer_idx, layer in enumerate(reversed(layers)):\n",
        "        layer.update_weights(learning_rate, row)\n",
        "      # error = log_loss([y_train.loc[rowIndex,'target']], [output], labels=[0.0, 1.0, 2.0])\n",
        "    print(\"Entropy based loss: %f\"% (sum_error))\n",
        "      # if error != 0:\n",
        "      #   layer1_error = layers[1].update_weights(error, layers[0].output, learning_rate)\n",
        "      #   layers[0].update_weights(layer1_error, row, learning_rate)\n",
        "    print(\"New Weights of output layer\", layers[1].w)\n",
        "    print(\"New Weights of hidden layer\", layers[0].w)\n",
        "    # for layer_idx, layer in enumerate(layers):\n",
        "    #   layer.update_weights()\n",
        "    index = index + 1\n",
        "  # df_plot = pd.DataFrame(df_plot, columns=['mse', 'epoch'])\n",
        "  return\n",
        "\n",
        "layers = reset_layers()\n",
        "print(\"Creating model using training data\")\n",
        "train_weights_using_mini_batch_gradient_descent(X_train, y_train, 10, 0.5, layers)"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating model using training data\n",
            "============================================\n",
            "=================Iteration 1===============\n",
            "============================================\n",
            "3.8894648932609908\n",
            "4.155187012494743\n",
            "3.7898532744437294\n",
            "error of output layer [0.05209496214113262, 0.09239967751794666, 0.045077465658441916]\n",
            "error of hidden layer [-0.924075000068667, -0.8584776336890476, -1.5093839020724384]\n",
            "-0.40723919817466525\n",
            "-0.3433495431769678\n",
            "-0.8234527517991035\n",
            "error of output layer [1.7693436286871802, 1.338666933343476, 1.697524444892754]\n",
            "error of hidden layer [-1.5172439754973923, -0.8369635748002537, -5.8390712558725095]\n",
            "-0.9314955841883636\n",
            "-0.564435611161566\n",
            "-3.306933206097113\n",
            "error of output layer [0.7557494110407533, 0.09115062068687452, 0.7173251205541585]\n",
            "error of hidden layer [-0.5357466039249934, 0.051367427788999435, -0.4125112120237271]\n",
            "-2.0306564403453\n",
            "-1.0682053726195073\n",
            "-6.259675881190513\n",
            "error of output layer [1.0830121924627834, 5.253338511948132e-06, 1.08300822133691]\n",
            "error of hidden layer [2.225941146048727, 1.3384653310653793, 120.75827894693508]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 12.356966\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 2===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 3===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 4===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 5===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 6===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 7===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 8===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 9===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n",
            "============================================\n",
            "=================Iteration 10===============\n",
            "============================================\n",
            "-0.038041739256295365\n",
            "2.0892884677180055\n",
            "1598.300968979236\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.05223842171416239\n",
            "2.295808102431347\n",
            "1118.7979603469876\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03565668819912831\n",
            "2.627883606956776\n",
            "2039.3986325000005\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.056377598157659616\n",
            "2.566402703403957\n",
            "1111.3029209753274\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03599479959578535\n",
            "2.911558085661898\n",
            "2203.287545961741\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03420889000793267\n",
            "2.4069596721076314\n",
            "1962.6058933924344\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.04994872326156283\n",
            "1.9206292616987755\n",
            "981.1327091283579\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.03935564418092366\n",
            "2.4187151274368524\n",
            "1749.0780513194206\n",
            "error of output layer [-0.0, -0.0, -0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "-0.052978212910521454\n",
            "2.2539744065273624\n",
            "1060.7338441108716\n",
            "error of output layer [0.0, 0.0, 0.0]\n",
            "error of hidden layer [0.0, 0.0, 0.0]\n",
            "Entropy based loss: 16.000000\n",
            "New Weights of output layer [[-1.0392980116070605, -0.9123590579154929, -4.824539201827845], [0.2795786334068177, 0.5176342577921346, 0.3866703832394163], [-1.4938962228723425, -0.8039253351287305, -4.408419890480031]]\n",
            "New Weights of hidden layer [[-0.1211795859583451, 0.07397911963590698], [0.666690289657174, 0.2091684795199238], [159.20226996502745, 290.3205811805799]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS3EIJKe6PyB",
        "outputId": "9ebdf2be-af13-4da9-9417-46ed048ce7c2"
      },
      "source": [
        "for rowIndex, row in X_test.iterrows():\n",
        "  # print(rowIndex)\n",
        "  output = row\n",
        "  for layer_idx, layer in enumerate(layers):\n",
        "      if layer_idx == len(layers) - 1:\n",
        "        output = layer.calculate_output(output, softmax)\n",
        "      else:\n",
        "        output = layer.calculate_output(output, relu)\n",
        "  # print(output)\n",
        "  print(\"Predicted: %d, Actual %d\"%(np.argmax(output), y_test.loc[rowIndex, 'target']))"
      ],
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 2\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 2\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 2\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 2\n",
            "Predicted: 1, Actual 2\n",
            "Predicted: 1, Actual 2\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 0\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 1\n",
            "Predicted: 1, Actual 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}