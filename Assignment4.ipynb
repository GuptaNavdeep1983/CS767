{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtHlZMp+OWi+B2CddYIZTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS767/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA3_HENeBQVT"
      },
      "source": [
        "import random\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWw7ta9INmAc"
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "def sigmoid(output):\n",
        "  return [1.0/(1.0+np.exp(-z)) for z in output]\n",
        "def relu(x):\n",
        "  output = [0] * len(x)\n",
        "  for index, item in enumerate(x): \n",
        "    output[index] = item if item > 0.0 else item * 0.05\n",
        "  return output"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRmzSqyQUEeN"
      },
      "source": [
        "def categorical_cross_entropy(actual, predicted):\n",
        "\tsum_score = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\t\tsum_score += actual[i] * math.log(1e-15 + predicted[i])\n",
        "\tmean_sum_score = (1.0 / len(actual)) * sum_score\n",
        "\treturn -mean_sum_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFLe9USlBDxa"
      },
      "source": [
        "output_labels = [0.0, 1.0, 2.0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vkOKqUPFY0t"
      },
      "source": [
        "'''\n",
        "Class for the layer of the network \n",
        "which manages weights, bias, output and errors  \n",
        "'''\n",
        "class Layer():\n",
        "  def __init__(self, name, units, input_dim):\n",
        "    self.name = name\n",
        "    self.w = [[random.uniform(0,1) for _ in range(input_dim)] for _ in range(units)]\n",
        "    self.b = [0 for _ in range(units)]\n",
        "    self.output = [bias for bias in self.b]\n",
        "    self.next_layer = None\n",
        "    self.previous_layer = None\n",
        "    self.error = [0 for _ in range(units)]\n",
        "  def set_previous_layer(self, previous_layer):\n",
        "    self.previous_layer = previous_layer\n",
        "  def set_next_layer(self, next_layer):\n",
        "    self.next_layer = next_layer\n",
        "  '''\n",
        "  function to calculate the output of the neurons at current layer with \n",
        "  the given inputs\n",
        "  '''\n",
        "  def calculate_output(self, inputs, activation=sigmoid):\n",
        "    self.output = [bias for bias in self.b]\n",
        "    self.output += np.dot(self.w, inputs)\n",
        "    self.output = activation(self.output)\n",
        "    return self.output \n",
        "  '''\n",
        "  function to calculate the error in the output at current layer with \n",
        "  the given expected output\n",
        "  '''\n",
        "  def update_error(self, expected_output):\n",
        "    # last layer\n",
        "    if self.next_layer == None:\n",
        "      inputs = self.previous_layer.output\n",
        "      # total_error = sum([(exp_output-self.output[i])**2 for i, exp_output in enumerate(expected_output)])\n",
        "      # total_error = log_loss(expected_output, [self.output], labels=output_labels)\n",
        "      total_error = categorical_cross_entropy(self.output, expected_output)\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        self.error[row_index] = total_error * self.output[row_index] * (1-self.output[row_index]) \n",
        "    # layers other than last layer\n",
        "    else:\n",
        "      weights = self.next_layer.w\n",
        "      next_layer_error = self.next_layer.error\n",
        "      for output_idx, output in enumerate(self.output):\n",
        "        self.error[output_idx] = sum([weight[output_idx] * next_layer_error[weight_idx] * output * (1-output) for weight_idx, weight in enumerate(weights)])\n",
        "  '''\n",
        "  function to update the weights at current layer with \n",
        "  the given inputs\n",
        "  '''\n",
        "  def update_weights(self, learning_rate, row):\n",
        "    if self.next_layer == None:\n",
        "      inputs = self.previous_layer.output\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        for col_index, weight in enumerate(col):\n",
        "          self.w[row_index][col_index] = self.w[row_index][col_index] + (learning_rate * self.error[row_index] * inputs[col_index])\n",
        "          self.b[row_index] += learning_rate * self.error[row_index]\n",
        "    else:\n",
        "      inputs = row\n",
        "      for row_index, col in enumerate(self.w):\n",
        "        for col_index, weight in enumerate(col):\n",
        "          self.w[row_index][col_index] = self.w[row_index][col_index] + (learning_rate * self.error[row_index] * inputs[col_index])\n",
        "          self.b[row_index] += learning_rate * self.error[row_index]\n",
        "  \n",
        "    "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLEqMj-uWIHC"
      },
      "source": [
        "# create network with the hidden and output layers\n",
        "def reset_layers(inputs, hidden_layer, output):\n",
        "  layer2 = Layer(\"Output\", output, hidden_layer)\n",
        "  layer1 = Layer(\"Hidden1\", hidden_layer, inputs)\n",
        "  layer2.set_previous_layer(layer1)\n",
        "  layer1.set_next_layer(layer2)\n",
        "  layers = [layer1, layer2]\n",
        "  return layers"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHMYuFAERXRP"
      },
      "source": [
        "# Load IRIS data\n",
        "iris = load_iris()\n",
        "\n",
        "iris_columns = ['sepal_len', 'sepal_width', 'petal_length', 'petal_width', 'target']\n",
        "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris_columns)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J359lR1SRnyu"
      },
      "source": [
        "# select desired columns, split into training and test datasets\n",
        "# standardize the inputs\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X = df[['sepal_len', 'sepal_width', 'petal_length', 'petal_width']]\n",
        "X = df[['sepal_width', 'petal_length']]\n",
        "y = df[['target']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state = 10)\n",
        "\n",
        "\n",
        "scaler_training = StandardScaler() \n",
        "scaler_training.fit(X_train)\n",
        "scaled_values_training = scaler_training.transform(X_train)\n",
        "scaled_values_testing = scaler_training.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(scaled_values_training, columns=[*X_train.columns.values])\n",
        "X_test = pd.DataFrame(scaled_values_testing, columns=[*X_test.columns.values])\n",
        "\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBdWTT0pSJU_"
      },
      "source": [
        "# set random seed\n",
        "import random\n",
        "random.seed(68)\n",
        "selected_rows = random.sample(range(0,119),   5)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmdWwns5U23f",
        "outputId": "ea53bcb9-2329-492d-c15b-12933c6746ac"
      },
      "source": [
        "# label binarizer\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit([0,1,2])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzwEWlmlRvLb",
        "outputId": "e3e7df13-8a2b-455b-a466-6bc404e7c628"
      },
      "source": [
        "def train_weights(X_train, y_train, num_epochs, learning_rate, layers):\n",
        "  index = 0;\n",
        "  # select mini batch\n",
        "  mini_batch = X_train.loc[selected_rows, :]\n",
        "  y_train = y_train.loc[selected_rows, :]\n",
        "  while index < num_epochs:\n",
        "    sum_error = 0\n",
        "    print(\"============================================\")\n",
        "    print(\"=================Iteration %d===============\"%(index+1))\n",
        "    print(\"============================================\")\n",
        "    for rowIndex, row in mini_batch.iterrows():\n",
        "      # pass the inputs and get the outputs in a typical feed forward network.\n",
        "      output = row\n",
        "      for layer_idx, layer in enumerate(layers):\n",
        "        if layer_idx == len(layers) - 1:\n",
        "          output = layer.calculate_output(output, softmax)\n",
        "        else:\n",
        "          output = layer.calculate_output(output, sigmoid)\n",
        "      # calculate total error \n",
        "      expected_output = lb.transform([y_train.loc[rowIndex,'target']])[0]\n",
        "      sum_error += sum([(exp_output-output[i])**2 for i, exp_output in enumerate(expected_output)])\n",
        "\n",
        "      # update error at each layer\n",
        "      for layer_idx, layer in enumerate(reversed(layers)):\n",
        "        # layer.update_error([y_train.loc[rowIndex,'target']])\n",
        "        layer.update_error(expected_output)\n",
        "      print(\"error of output layer\", layers[1].error)\n",
        "      print(\"error of hidden layer\", layers[0].error)  \n",
        "      # update weights at each layer\n",
        "      for layer_idx, layer in enumerate(reversed(layers)):\n",
        "        layer.update_weights(learning_rate, row)\n",
        "      \n",
        "      # error = log_loss([y_train.loc[rowIndex,'target']], [output], labels=[0.0, 1.0, 2.0])\n",
        "    print(\"Entropy based loss: %f\"% (sum_error))\n",
        "      # if error != 0:\n",
        "      #   layer1_error = layers[1].update_weights(error, layers[0].output, learning_rate)\n",
        "      #   layers[0].update_weights(layer1_error, row, learning_rate)\n",
        "    print(\"Weights of output layer\", layers[1].w)\n",
        "    print(\"Weights of hidden layer\", layers[0].w)\n",
        "    index = index + 1\n",
        "\n",
        "layers = reset_layers(2, 3, 3)\n",
        "print(\"Creating model using training data\")\n",
        "train_weights(X_train, y_train, 2, 0.5, layers)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating model using training data\n",
            "============================================\n",
            "=================Iteration 1===============\n",
            "============================================\n",
            "error of output layer [1.7368157915950448, 1.6767918348107558, 1.7056146479985879]\n",
            "error of hidden layer [0.6942278677700293, 0.5730120695440293, 0.7657205268975965]\n",
            "error of output layer [1.8302220188740579, 1.6428293788645632, 1.7042421855178111]\n",
            "error of hidden layer [0.828468892453033, 1.0340210187181174, 1.105484380183395]\n",
            "error of output layer [1.9884675466869537, 1.4681775440283094, 1.695084394989339]\n",
            "error of hidden layer [1.1124230745194597, 1.3690902880309834, 1.174936691979572]\n",
            "error of output layer [1.0542447172974283, 0.45675343678025687, 0.8766261243918505]\n",
            "error of hidden layer [1.5411237956848653, 0.8275064499661879, 0.9730990758692253]\n",
            "error of output layer [1.7351031152199201, 0.2488981079514728, 1.571695262110323]\n",
            "error of hidden layer [0.0651524343309799, 0.08310206865349759, 0.07678088919461337]\n",
            "Entropy based loss: 3.984325\n",
            "Weights of output layer [[4.156781889633132, 3.099426532442715, 3.729289936886762], [2.4684909992356836, 2.0180246179166725, 2.7469105278421604], [3.263805051495534, 3.6563107518711266, 3.188175082695212]]\n",
            "Weights of hidden layer [[-0.9417893437329538, 0.6316151446655528], [-0.9492726878620275, 0.5995522935685605], [-0.8422312096253042, 0.5778627015030131]]\n",
            "============================================\n",
            "=================Iteration 2===============\n",
            "============================================\n",
            "error of output layer [1.027162960310427, 0.0030612902221982107, 1.0247948543482763]\n",
            "error of hidden layer [0.06410318305534358, 0.08396938029973458, 0.07221832887467458]\n",
            "error of output layer [1.018584345173293, 0.0001433370309042086, 1.0184731873798123]\n",
            "error of hidden layer [0.040495272742188046, 0.05312521590718063, 0.04776778208632361]\n",
            "error of output layer [1.0164963096763695, 6.682386771067078e-06, 1.0164911235643286]\n",
            "error of hidden layer [0.01141316159688157, 0.014476921908250836, 0.015455501613707377]\n",
            "error of output layer [0.13260737250183086, 5.287035002052569e-08, 0.13260733168797217]\n",
            "error of hidden layer [0.03939711625077466, 0.04823937760351442, 0.04010463355632883]\n",
            "error of output layer [1.1471858470858345, 2.513678443484721e-07, 1.147185652145758]\n",
            "error of hidden layer [0.05976613758003939, 0.07621069439856237, 0.06973901700996502]\n",
            "Entropy based loss: 6.552490\n",
            "Weights of output layer [[6.315655421439007, 5.253455550630689, 5.8855849085937075], [2.470083478197907, 2.0196111748311862, 2.74850018443846], [5.421446551209048, 5.809112319338124, 5.343240205765057]]\n",
            "Weights of hidden layer [[-0.9888879205571168, 0.6586774495178197], [-1.0095691934994333, 0.6369396999869577], [-0.8987402630469553, 0.6141190978597395]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFbrih6qplAA"
      },
      "source": [
        "# select desired columns, split into training and test datasets\n",
        "# standardize the inputs\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df[['sepal_len', 'sepal_width', 'petal_length', 'petal_width']]\n",
        "y = df[['target']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state = 10)\n",
        "\n",
        "\n",
        "scaler_training = StandardScaler() \n",
        "scaler_training.fit(X_train)\n",
        "scaled_values_training = scaler_training.transform(X_train)\n",
        "scaled_values_testing = scaler_training.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(scaled_values_training, columns=[*X_train.columns.values])\n",
        "X_test = pd.DataFrame(scaled_values_testing, columns=[*X_test.columns.values])\n",
        "\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zoWikiSuD43"
      },
      "source": [
        "# set random seed\n",
        "import random\n",
        "random.seed(68)\n",
        "selected_rows = random.sample(range(0,119), 10)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHyOwDDVpDBu",
        "outputId": "ebf96aca-aa8f-4622-d59b-f2a52654d059"
      },
      "source": [
        "layers = reset_layers(4, 5, 3)\n",
        "train_weights(X_train, y_train, 2, 0.5, layers)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================\n",
            "=================Iteration 1===============\n",
            "============================================\n",
            "error of output layer [1.0786260194400918, 1.2831662785205549, 1.5200857289182272]\n",
            "error of hidden layer [0.43443858452978223, 0.41460107095467885, 0.3389054332104148, 0.5265562539556266, 0.13913829338856626]\n",
            "error of output layer [0.1367448163016792, 0.3970587082474105, 0.4817887444450306]\n",
            "error of hidden layer [0.09301873563781757, 0.0675850030477192, 0.05825835800504621, 0.06479257903013767, 0.09328441175521114]\n",
            "error of output layer [0.03055085002042671, 0.24357555825980234, 0.2648197988593317]\n",
            "error of hidden layer [0.1261248288709685, 0.0853724660882837, 0.09749618441777483, 0.13671432559041924, 0.10467650996252573]\n",
            "error of output layer [0.49433410624364826, 2.213802378632092, 2.4170038899349837]\n",
            "error of hidden layer [0.7469677654062885, 0.30214602045469663, 0.1348877567495412, 0.4051506783034388, 0.42078433584048336]\n",
            "error of output layer [0.00031790403248639865, 0.7791763099539799, 0.7794433783706572]\n",
            "error of hidden layer [0.1462412788424765, 0.12334902297290037, 0.14285491387586782, 0.1323902949747828, 0.22436698130136468]\n",
            "error of output layer [1.211042681949533e-05, 0.8664018724420723, 0.8664117764539258]\n",
            "error of hidden layer [0.1307203398899891, 0.3886807005797341, 0.5419301449225093, 0.3242564195634195, 0.2748955639410203]\n",
            "error of output layer [8.809531900165498e-09, 0.05555130340463207, 0.05555131094362134]\n",
            "error of hidden layer [0.006304289961143494, 0.0022717040296392097, 0.0008807447768148899, 0.0017851580471201958, 0.004563755906605742]\n",
            "error of output layer [9.372827018429211e-08, 0.7282856058178113, 0.7282856857250398]\n",
            "error of hidden layer [0.07388169972584038, 0.07802464367979396, 0.056270095755063254, 0.05366665039147917, 0.10703003097831609]\n",
            "error of output layer [7.589261882937007e-08, 1.494235452110526, 1.4942355047369782]\n",
            "error of hidden layer [0.1596454985294755, 1.8613020807391907, 1.1974088087900399, 1.766599253611047, 1.0198482622027636]\n",
            "error of output layer [5.315301456258736e-12, 0.7668885116415209, 0.7668885116460731]\n",
            "error of hidden layer [0.054064109412922015, 0.00015273827912536014, 0.017756310852770962, 0.00014397485605368082, 0.004141051197142007]\n",
            "Entropy based loss: 10.680957\n",
            "Weights of output layer [[1.3151009569616563, 0.9350712493297879, 0.8036184349758362, 0.586107418734446, 0.5938132037275438], [3.669790582763327, 3.2468905991505825, 2.6078422194793096, 3.727857853809593, 2.8906207758190794], [3.3605731788643265, 3.2638996913749674, 3.5034355209603083, 3.880993101512878, 3.3289407299983287]]\n",
            "Weights of hidden layer [[-0.3815116332003709, 0.5485458807022808, 0.1904454501588368, -0.08261429158876311], [-0.6681694257395915, 1.0456720467780876, -0.7759139040575377, -0.778183376945228], [-0.09320232254908066, 0.6835686645258541, -0.010084633830106285, 0.23114011361262346], [-0.930235714282457, 1.3214827626233396, -0.4337723242504627, -0.7273354781287739], [-0.9381550864453733, 0.8024401715546193, 0.06230466641713334, -0.3187922389290682]]\n",
            "============================================\n",
            "=================Iteration 2===============\n",
            "============================================\n",
            "error of output layer [9.067595245752508e-15, 0.05542594736512325, 0.055425947365131]\n",
            "error of hidden layer [0.008037711610781057, 0.001719389078635297, 0.0018251983607054717, 0.0014562157593873185, 0.004549187103455263]\n",
            "error of output layer [7.356787133877348e-15, 0.055849619907526224, 0.05584961990753248]\n",
            "error of hidden layer [0.009476168525746436, 0.003940338883928358, 0.002241497224186024, 0.0033589505150521896, 0.007360878457495267]\n",
            "error of output layer [7.511974561539415e-15, 0.05785143364726601, 0.057851433647272424]\n",
            "error of hidden layer [0.01995945062674537, 0.00960718744252388, 0.007216603091699978, 0.013141575278563081, 0.016838782028330328]\n",
            "error of output layer [5.089988712167147e-14, 0.7685333572461028, 0.7685333572461461]\n",
            "error of hidden layer [0.06418001556171496, 0.00022497010732129504, 0.03454954388869515, 0.00025258284352715373, 0.005434962344238573]\n",
            "error of output layer [1.2280605425585613e-15, 0.717906551350869, 0.7179065513508693]\n",
            "error of hidden layer [0.15971626992272106, 0.039715471814047096, 0.0405626545251399, 0.045995359870028854, 0.13666232990378246]\n",
            "error of output layer [3.014669575982988e-17, 0.7137846616693113, 0.7137846616693114]\n",
            "error of hidden layer [0.07615126721271266, 0.008257872551348073, 0.03194356745458156, 0.00813660527103828, 0.034516607465231254]\n",
            "error of output layer [7.52643003983759e-20, 0.055773507806647846, 0.055773507806647894]\n",
            "error of hidden layer [0.006535353146439089, 0.006714769629677398, 0.001982348118337804, 0.005044975777214208, 0.007317870958733308]\n",
            "error of output layer [6.441124500427514e-19, 0.7130943231341764, 0.7130943231341775]\n",
            "error of hidden layer [0.05815505221716086, 0.012590205524313492, 0.01835348470567732, 0.009459924316687778, 0.03879920635972017]\n",
            "error of output layer [1.8735050804036436e-20, 0.7670169077190626, 0.7670169077190626]\n",
            "error of hidden layer [0.027170601558602868, 9.873464830587623e-05, 0.015010786041111123, 8.086942691622771e-05, 0.003186892329175085]\n",
            "error of output layer [4.088951975876162e-22, 0.7673989098127633, 0.7673989098127638]\n",
            "error of hidden layer [0.03206663093932261, 0.00025759167290421514, 0.02101795100427088, 0.00022492223879024448, 0.005680178584897118]\n",
            "Entropy based loss: 10.807747\n",
            "Weights of output layer [[1.3151009569616936, 0.9350712493298258, 0.8036184349758739, 0.5861074187344839, 0.5938132037275813], [5.977759147787237, 5.5777090169704815, 4.932420310848477, 6.059070934944242, 5.208955116113302], [5.668541743888269, 5.594718109194899, 5.828013612329508, 6.21220618264756, 5.647275070292584]]\n",
            "Weights of hidden layer [[-0.4129442089294135, 0.49841837625858104, 0.16680999472952984, -0.10623883183534391], [-0.6473775200019932, 1.023777059323896, -0.7547249927995481, -0.7596307981945657], [-0.13352734292919616, 0.6754162782921539, -0.04301894031824581, 0.20008414531200552], [-0.9083813945373956, 1.2934966549401834, -0.4120358959333792, -0.7091392448517909], [-0.8872255505240815, 0.7468905190000605, 0.10697474425154349, -0.2800132625387674]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}